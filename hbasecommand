curl -O https://people.apache.org/~jdcryans/word_count.csv https://people.apache.org/~jdcryans/word_count.csv

hadoop fs -put <src_data_dir> <dir_in_hdfs>

create 'twitterdata', {NAME => 'family', VERSION => 100}


hadoop jar hbase-0.94.6-cdh4.3.0-security.jar importtsv -Dimporttsv.separator='\t' -Dimporttsv.bulk.output=output -Dimporttsv.columns=HBASE_ROW_KEY,HBASE_ROW_KEY,family:tweet_id,family:sentiment,family:tweet twitterdata out
bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,family:tweet_id,family:sentiment,family:tweet twitterdata3 /out-part-00000
// 不加output选项可以直接输出到hbase中，否则输出到hdfs中


put 'twitterdata', 'row1', 'family:user_id', '1234567890'
put 'twitterdata', 'row1', 'family:tweet_id', '123456789012345678901234567890'
put 'twitterdata', 'row1', 'family:sentiment', '0'


hadoop fs -put out .
