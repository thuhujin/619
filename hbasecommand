hadoop fs -put out* data/
hadoop fs -mkdir -p data

hadoop fs -put <src_data_dir> <dir_in_hdfs>
create 'twitterdata', {NAME => 'family', VERSION => 5}
bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,family:tweet_id,family:sentiment,family:tweet twitterdata /out-part-000*
bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,family:tweet twitterdata data/final-part-000*
sudo java -jar Hbaseclass.jar <dns-name> <password_not_required>

// hadoop jar hbase-0.94.6-cdh4.3.0-security.jar importtsv -Dimporttsv.separator='\t' -Dimporttsv.bulk.output=output -Dimporttsv.columns=HBASE_ROW_KEY,HBASE_ROW_KEY,family:tweet_id,family:sentiment,family:tweet twitterdata out
// if no output dir is specified, output to hbase directly


// put 'twitterdata', 'row1', 'family:user_id', '1234567890'
// put 'twitterdata', 'row1', 'family:tweet_id', '123456789012345678901234567890'
// put 'twitterdata', 'row1', 'family:sentiment', '0'
